{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8b5bc70-3f6d-4696-9663-cb70a6081fe3",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c179774f-6d22-4285-9433-5e7f6b16b2c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# Load graph data and labels\n",
    "# Set working directory and define input/output paths\n",
    "work_dir = os.getcwd()  # Set current directory as work_dir\n",
    "input_data_dir = os.path.join(work_dir, '../Data')  # Define input data path as ../Data\n",
    "output_dir = os.path.join(work_dir, '../Data')  # Define output path as ../Data\n",
    "\n",
    "# Load merged graph data from file\n",
    "merged_file = os.path.join(input_data_dir, 'all_graphs_to_be_predicted.pt')\n",
    "merged_graphs = torch.load(merged_file)\n",
    "n = len(merged_graphs)  # Count number of graphs loaded\n",
    "n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d009b4-4457-4659-b6cc-9eb786f6894f",
   "metadata": {},
   "source": [
    "# GAT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "224152d0-c843-4c03-b9a7-54b965c599e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GATModel(\n",
      "  (layer1): GATConv(91, 64, heads=4)\n",
      "  (layer2): GATConv(256, 64, heads=4)\n",
      "  (layer3): GATConv(256, 64, heads=1)\n",
      "  (fc): Linear(in_features=64, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GATConv, global_mean_pool\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "def set_seed(seed):\n",
    "    import random\n",
    "    import numpy as np\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  # For GPU\n",
    "\n",
    "set_seed(42)  # Set random seed\n",
    "\n",
    "\n",
    "# Define the GAT-based model\n",
    "class GATModel(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, num_heads, dropout_rate=0.3, dosage_weight=1.0, initialize_weights=True):\n",
    "        super(GATModel, self).__init__()\n",
    "        self.dosage_weight = dosage_weight  # Scaling factor for the 91st feature\n",
    "        self.layer1 = GATConv(in_dim, hidden_dim, heads=num_heads, dropout=dropout_rate)\n",
    "        self.layer2 = GATConv(hidden_dim * num_heads, hidden_dim, heads=num_heads, dropout=dropout_rate)\n",
    "        self.layer3 = GATConv(hidden_dim * num_heads, hidden_dim, heads=1, dropout=dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "        # Initialize weights if specified\n",
    "        if initialize_weights:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for layer in [self.layer1, self.layer2, self.layer3]:\n",
    "            nn.init.xavier_uniform_(layer.lin.weight)  # Initialize linear layer weights\n",
    "            if layer.lin.bias is not None:\n",
    "                nn.init.zeros_(layer.lin.bias)  # Initialize biases to 0\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        # Apply GAT layers with attention weights\n",
    "        h, attn_weights_1 = self.layer1(x, edge_index, return_attention_weights=True)\n",
    "        h = torch.relu(h)\n",
    "\n",
    "        h, attn_weights_2 = self.layer2(h, edge_index, return_attention_weights=True)\n",
    "        h = torch.relu(h)\n",
    "\n",
    "        h, attn_weights_3 = self.layer3(h, edge_index, return_attention_weights=True)\n",
    "        \n",
    "        # Aggregate node information using global mean pooling\n",
    "        hg = global_mean_pool(h, batch)\n",
    "        out = self.fc(hg)\n",
    "        \n",
    "        return out, hg, (attn_weights_1, attn_weights_2, attn_weights_3)  # Return output and attention weights\n",
    "\n",
    "\n",
    "# Model parameter configuration\n",
    "in_dim = 91       # Node feature dimension\n",
    "hidden_dim = 64   # Hidden layer dimension\n",
    "out_dim = 5       # Output dimension for 5 labels\n",
    "num_heads = 4     # Number of attention heads in GAT layers\n",
    "dropout_rate = 0.5\n",
    "dosage_weight = 1 # Scaling factor for dosage feature\n",
    "\n",
    "# Instantiate the GAT model\n",
    "model = GATModel(in_dim, hidden_dim, out_dim, num_heads, dropout_rate, dosage_weight=dosage_weight)\n",
    "print(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1fe649-e3b0-4b9c-a0e5-0386f273d014",
   "metadata": {},
   "source": [
    "# Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c2fadf07-ffde-4331-a2f4-4c17c6992252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "# Delete the previous model to free memory\n",
    "del model\n",
    "gc.collect()  # Force garbage collection\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e5fa03a-f96e-40c2-93ac-12c087b111f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully from D:\\博士文件\\TCMMKG\\GraphAI-for-TCM\\Python\\../Model\\gat_model.pth\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# Set the working directory and define input/output paths\n",
    "work_dir = os.getcwd()  # Set the current directory as work_dir\n",
    "input_data_dir = os.path.join(work_dir, '../Model')  # Define the input data path\n",
    "output_dir = os.path.join(work_dir, '../Data')  # Set ../Data as input data location\n",
    "\n",
    "# Load graph data and labels from input_data_dir (if applicable, add specific file loading here)\n",
    "\n",
    "# Define the path to the pre-trained GAT model\n",
    "model_save_path = os.path.join(input_data_dir, 'gat_model.pth')  # Use input_data_dir for path consistency\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = torch.load(model_save_path)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "print(f\"Model loaded successfully from {model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a55c365-6015-4ab9-9d44-ca87ef6476e3",
   "metadata": {},
   "source": [
    "# Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49714928-b815-471c-888c-22d224a12c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction outputs exported to D:\\博士文件\\TCMMKG\\GraphAI-for-TCM\\Python\\../Data\\prediction_outputs.tsv as TSV\n",
      "Attention weights exported to D:\\博士文件\\TCMMKG\\GraphAI-for-TCM\\Python\\../Data\\attention_weights.tsv as TSV\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# Define a helper function to format values to four significant figures\n",
    "def format_value(val):\n",
    "    return round(val, 4)\n",
    "\n",
    "# Prediction function to process a range of samples\n",
    "def predict_samples(start_index, end_index):\n",
    "    output_results = []\n",
    "    attn_results = []\n",
    "\n",
    "    for i in range(start_index, end_index + 1):\n",
    "        sample = merged_graphs[i]\n",
    "        cpm_id = sample.cpm_id  # Unique identifier for each sample\n",
    "        out, hg, attn_weights = model(sample)\n",
    "\n",
    "        # First output table: cpm_id, output (out), and graph-level embeddings (hg)\n",
    "        output_result = {\n",
    "            \"cpm_id\": cpm_id,\n",
    "            **{f\"out_{j+1}\": format_value(val) for j, val in enumerate(out.detach().cpu().numpy().flatten())},\n",
    "            **{f\"hg_{j+1}\": format_value(val) for j, val in enumerate(hg.detach().cpu().numpy().flatten())}\n",
    "        }\n",
    "        output_results.append(output_result)\n",
    "\n",
    "        # Second output table: cpm_id, node names, and attention weights\n",
    "        node_names = sample.node_names  # Ensure that the graph has a `node_names` attribute\n",
    "\n",
    "        # Dictionary to store edges and associated attention weights\n",
    "        edge_dict = {}\n",
    "\n",
    "        # Loop through each attention weight layer\n",
    "        for layer_idx, (edge_index, attn_weight) in enumerate(attn_weights, start=1):\n",
    "            edge_index_np = edge_index.detach().cpu().numpy()  # Shape [2, E]\n",
    "            attn_weight_np = attn_weight.detach().cpu().numpy()  # Shape [E, heads]\n",
    "\n",
    "            # Transpose edge_index_np to [E, 2]\n",
    "            edges = edge_index_np.T\n",
    "\n",
    "            # Extract node names and attention weights for each edge\n",
    "            for edge, attn in zip(edges, attn_weight_np):\n",
    "                node_idx_1, node_idx_2 = edge\n",
    "                node_name_1 = node_names[int(node_idx_1)]\n",
    "                node_name_2 = node_names[int(node_idx_2)]\n",
    "\n",
    "                edge_key = (node_name_1, node_name_2)\n",
    "\n",
    "                # Initialize dictionary entry for each edge\n",
    "                if edge_key not in edge_dict:\n",
    "                    edge_dict[edge_key] = {\n",
    "                        \"cpm_id\": cpm_id,\n",
    "                        \"Source\": node_name_1,\n",
    "                        \"Target\": node_name_2\n",
    "                    }\n",
    "\n",
    "                # Unroll attention weights, storing each head in a separate column\n",
    "                for head_idx, attn_value in enumerate(attn, start=1):\n",
    "                    attn_col_name = f\"attn_weights_{layer_idx}_head_{head_idx}\"\n",
    "                    edge_dict[edge_key][attn_col_name] = format_value(attn_value)\n",
    "\n",
    "        # Append edge dictionary values to attn_results\n",
    "        attn_results.extend(edge_dict.values())\n",
    "\n",
    "    # Export the first table to a TSV file\n",
    "    output_df = pd.DataFrame(output_results)\n",
    "    output_path = os.path.join(output_dir, 'prediction_outputs.tsv')\n",
    "    output_df.to_csv(output_path, sep='\\t', index=False)\n",
    "    print(f\"Prediction outputs exported to {output_path} as TSV\")\n",
    "\n",
    "    # Export the second table to a TSV file\n",
    "    attn_df = pd.DataFrame(attn_results)\n",
    "    attn_path = os.path.join(output_dir, 'attention_weights.tsv')\n",
    "    attn_df.to_csv(attn_path, sep='\\t', index=False)\n",
    "    print(f\"Attention weights exported to {attn_path} as TSV\")\n",
    "\n",
    "# Define a custom range for prediction\n",
    "start_index = 0  # Starting index\n",
    "end_index = n - 1  # Ending index (can be changed as needed)\n",
    "predict_samples(start_index, end_index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0839f1-1f1c-4870-b7a5-4425c3114742",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
