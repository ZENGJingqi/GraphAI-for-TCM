{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78fce8d8-21ba-4977-97b3-0b6608f579e5",
   "metadata": {},
   "source": [
    "# Dataset Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf235c8-0d03-4da5-859b-cf1119a36f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set working directory and define paths for input and output data\n",
    "work_dir = os.getcwd()  # Use the current directory as work_dir\n",
    "input_data_dir = os.path.join(work_dir, '../Data')  # Set ../Data as input data location\n",
    "output_dir = os.path.join(work_dir, '../Data')  # Set ../Data as input data location\n",
    "\n",
    "# Load the merged graph dataset with labels\n",
    "merged_file = os.path.join(input_data_dir, 'all_graphs_with_labels-train.pt')\n",
    "merged_graphs = torch.load(merged_file)\n",
    "\n",
    "# Extract labels from each graph in the dataset, converting to NumPy array\n",
    "labels = np.array([graph.y.numpy() if isinstance(graph.y, torch.Tensor) else graph.y for graph in merged_graphs])\n",
    "\n",
    "# Function to randomly split data into training and testing sets\n",
    "def random_train_test_split(graphs, labels, test_size=0.3, random_state=42):\n",
    "    \"\"\"Split graphs and labels into training and test sets using random split\"\"\"\n",
    "    train_graphs, test_graphs, train_labels, test_labels = train_test_split(\n",
    "        graphs, labels, test_size=test_size, random_state=random_state, shuffle=True)\n",
    "\n",
    "    return train_graphs, test_graphs, train_labels, test_labels\n",
    "\n",
    "# Split dataset into training (70%) and temporary (30%) sets\n",
    "train_graphs, temp_graphs, train_labels, temp_labels = random_train_test_split(\n",
    "    merged_graphs, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "# Further split the temporary set (30% of original) into validation (20%) and test sets (10%)\n",
    "val_graphs, test_graphs, val_labels, test_labels = random_train_test_split(\n",
    "    temp_graphs, temp_labels, test_size=0.33, random_state=42)\n",
    "\n",
    "# Function to calculate the proportion of '1's in each label across the labels dataset\n",
    "def calculate_label_proportions(labels):\n",
    "    proportions = np.mean(labels == 1, axis=0)  # Calculate the proportion of '1's for each label\n",
    "    return proportions\n",
    "\n",
    "# Calculate the proportion of '1's in each subset's labels\n",
    "train_proportions = calculate_label_proportions(train_labels)\n",
    "val_proportions = calculate_label_proportions(val_labels)\n",
    "test_proportions = calculate_label_proportions(test_labels)\n",
    "\n",
    "# Convert labels to torch.Tensor format for compatibility with PyTorch models\n",
    "train_labels = torch.tensor(train_labels)\n",
    "val_labels = torch.tensor(val_labels)\n",
    "test_labels = torch.tensor(test_labels)\n",
    "\n",
    "# Print the size of each subset\n",
    "print(f\"Training set: {len(train_graphs)} graphs\")\n",
    "print(f\"Validation set: {len(val_graphs)} graphs\")\n",
    "print(f\"Test set: {len(test_graphs)} graphs\")\n",
    "\n",
    "# Print the proportion of '1's in each label for each subset\n",
    "print(\"Proportion of '1's for each label in training set:\", train_proportions)\n",
    "print(\"Proportion of '1's for each label in validation set:\", val_proportions)\n",
    "print(\"Proportion of '1's for each label in test set:\", test_proportions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d009b4-4457-4659-b6cc-9eb786f6894f",
   "metadata": {},
   "source": [
    "# GAT Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "224152d0-c843-4c03-b9a7-54b965c599e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GATModel(\n",
      "  (layer1): GATConv(91, 64, heads=4)\n",
      "  (layer2): GATConv(256, 64, heads=4)\n",
      "  (layer3): GATConv(256, 64, heads=1)\n",
      "  (fc): Linear(in_features=64, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GATConv, global_mean_pool\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "def set_seed(seed):\n",
    "    import random\n",
    "    import numpy as np\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  # For GPU, if used\n",
    "\n",
    "set_seed(42)  # Set the random seed\n",
    "\n",
    "# Define the GAT-based model\n",
    "class GATModel(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, num_heads, dropout_rate=0.3, dosage_weight=1.0):\n",
    "        super(GATModel, self).__init__()\n",
    "        self.dosage_weight = dosage_weight  # Controls amplification of the 91st feature\n",
    "        self.layer1 = GATConv(in_dim, hidden_dim, heads=num_heads, dropout=dropout_rate)\n",
    "        self.layer2 = GATConv(hidden_dim * num_heads, hidden_dim, heads=num_heads, dropout=dropout_rate)\n",
    "        self.layer3 = GATConv(hidden_dim * num_heads, hidden_dim, heads=1, dropout=dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        # Initialize weights for each layer using Xavier initialization\n",
    "        for layer in [self.layer1, self.layer2, self.layer3]:\n",
    "            nn.init.xavier_uniform_(layer.lin.weight)  # Linear layer weight initialization\n",
    "            if layer.lin.bias is not None:\n",
    "                nn.init.zeros_(layer.lin.bias)  # Initialize bias to 0\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        \n",
    "        # Amplify the 91st feature (dosage feature) within a range of 0-10\n",
    "        x[:, 90] = torch.clamp(x[:, 90] * self.dosage_weight, min=0, max=10)\n",
    "\n",
    "        # Perform GAT layers computations and obtain attention weights\n",
    "        h, attn_weights_1 = self.layer1(x, edge_index, return_attention_weights=True)\n",
    "        h = torch.relu(h)\n",
    "\n",
    "        h, attn_weights_2 = self.layer2(h, edge_index, return_attention_weights=True)\n",
    "        h = torch.relu(h)\n",
    "\n",
    "        h, attn_weights_3 = self.layer3(h, edge_index, return_attention_weights=True)\n",
    "        \n",
    "        # Global mean pooling to aggregate node information into a graph-level representation\n",
    "        hg = global_mean_pool(h, batch)\n",
    "        out = self.fc(hg)\n",
    "        \n",
    "        # Return output, pooled node features, and attention weights from each layer\n",
    "        return out, hg, (attn_weights_1, attn_weights_2, attn_weights_3)\n",
    "\n",
    "\n",
    "# Model parameter configuration\n",
    "in_dim = 91       # Input dimension for node features\n",
    "hidden_dim = 64   # Dimension of hidden layers\n",
    "out_dim = 5       # Output dimension, corresponding to 5 labels\n",
    "num_heads = 4     # Number of attention heads in GAT layers\n",
    "dropout_rate = 0.5\n",
    "dosage_weight = 1  # Amplification weight for dosage feature\n",
    "\n",
    "# Instantiate the GAT model\n",
    "model = GATModel(in_dim, hidden_dim, out_dim, num_heads, dropout_rate, dosage_weight=dosage_weight)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9120c7c-07b6-48df-a0d0-3d01146bda59",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3761c9-d98b-42d6-ae44-720f93866030",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score\n",
    "from torch_geometric.loader import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Compute class weights to handle label imbalance\n",
    "num_classes = train_labels.size(1)\n",
    "pos_counts = train_labels.sum(dim=0)\n",
    "neg_counts = train_labels.size(0) - pos_counts\n",
    "pos_weight = neg_counts / (pos_counts + 1e-6)\n",
    "\n",
    "# Loss function, optimizer, and learning rate scheduler\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "learning_rate = 0.0001\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "\n",
    "# Early stopping class\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_loss = np.inf\n",
    "        self.counter = 0\n",
    "\n",
    "    def check_early_stop(self, val_loss):\n",
    "        if self.best_loss - val_loss > self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "        return self.counter >= self.patience\n",
    "\n",
    "# Print current learning rate for tracking adjustments\n",
    "def print_learning_rate(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        print(f\"Current Learning Rate: {param_group['lr']}\")\n",
    "\n",
    "# Custom collate function to directly return batches without additional processing\n",
    "def custom_collate(batch):\n",
    "    return batch  # Return original batch as is\n",
    "\n",
    "# Create data loader for batching graphs and labels\n",
    "def create_batches(graphs, labels, batch_size):\n",
    "    for i, graph in enumerate(graphs):\n",
    "        graph.y = labels[i]  # Attach labels to graph data\n",
    "    data_loader = DataLoader(graphs, batch_size=batch_size, shuffle=True, collate_fn=custom_collate)\n",
    "    return data_loader\n",
    "\n",
    "# Model evaluation function\n",
    "def evaluate_model(val_loader, model, loss_fn):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    all_preds, all_labels = [], []\n",
    "    all_attn_weights = []  # Collect attention weights\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            output, _, attn_weights = model(batch)\n",
    "\n",
    "            # Adjust label shape\n",
    "            batch_labels = batch.y.view(output.shape)\n",
    "            loss = loss_fn(output, batch_labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            preds = torch.round(torch.sigmoid(output))\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_labels.append(batch_labels.cpu().numpy())\n",
    "            all_attn_weights.append(attn_weights)  # Store attention weights\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    all_preds = np.vstack(all_preds)\n",
    "    all_labels = np.vstack(all_labels)\n",
    "\n",
    "    # Calculate recall and F1 scores\n",
    "    recall = recall_score(all_labels, all_preds, average='micro')\n",
    "    f1 = f1_score(all_labels, all_preds, average='micro')\n",
    "\n",
    "    return avg_val_loss, recall, f1, all_attn_weights  # Return attention weights for analysis\n",
    "\n",
    "# Training function for the model\n",
    "def train_model(train_graphs, train_labels, val_graphs, val_labels, model, loss_fn, optimizer, scheduler, num_epochs=50, batch_size=16, early_stopping_patience=5, grad_clip_value=1.0):\n",
    "    train_loader = create_batches(train_graphs, train_labels, batch_size)\n",
    "    val_loader = create_batches(val_graphs, val_labels, batch_size)\n",
    "\n",
    "    early_stopping = EarlyStopping(patience=early_stopping_patience)\n",
    "    val_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "            # Forward pass\n",
    "            output, _, _ = model(batch)  # Ignore attention weights during training\n",
    "\n",
    "            # Adjust label shape\n",
    "            batch_labels = batch.y.view(output.shape)\n",
    "            loss = loss_fn(output, batch_labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip_value)\n",
    "\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Training Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # Validate the model\n",
    "        val_loss, val_recall, val_f1, attn_weights = evaluate_model(val_loader, model, loss_fn)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Validation Loss: {val_loss:.4f}, Validation Recall: {val_recall:.4f}, Validation F1: {val_f1:.4f}\")\n",
    "\n",
    "        # Step the learning rate scheduler and print the current learning rate\n",
    "        scheduler.step(val_loss)\n",
    "        print_learning_rate(optimizer)\n",
    "\n",
    "        # Check early stopping\n",
    "        if early_stopping.check_early_stop(val_loss):\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "# Train the model\n",
    "train_model(train_graphs, train_labels, val_graphs, val_labels, model, loss_fn, optimizer, scheduler, num_epochs=30, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a55c365-6015-4ab9-9d44-ca87ef6476e3",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49714928-b815-471c-888c-22d224a12c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_fscore_support, accuracy_score, confusion_matrix\n",
    "import pandas as pd\n",
    "import os\n",
    "from matplotlib import rcParams\n",
    "\n",
    "# Set global font to Arial for consistent plots\n",
    "rcParams['font.family'] = 'Arial'\n",
    "\n",
    "# Function to evaluate the model on a given dataset and optionally output attention weights\n",
    "def evaluate_model(graphs, labels, model, output_dir, data_name, cpm_id=None):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    all_outputs = []\n",
    "    all_labels = []\n",
    "    all_attn_weights = []\n",
    "\n",
    "    # Collect predictions, labels, and attention weights for the dataset\n",
    "    with torch.no_grad():\n",
    "        for i, graph in enumerate(graphs):\n",
    "            output, hg, attn_weights = model(graph)  # Model returns output and attention weights\n",
    "            all_outputs.append(output.cpu().numpy())\n",
    "            all_labels.append(labels[i].cpu().numpy())\n",
    "            all_attn_weights.append(attn_weights)  # Store attention weights for each graph\n",
    "\n",
    "    final_outputs = np.vstack(all_outputs)\n",
    "    final_labels = np.vstack(all_labels)\n",
    "\n",
    "    # Calculate and save performance metrics\n",
    "    compute_and_save_metrics(final_labels, final_outputs, output_dir, data_name)\n",
    "\n",
    "    # Save attention weights for a specific `cpm_id`, if provided\n",
    "    if cpm_id is not None:\n",
    "        output_attention_weights(all_attn_weights, graphs, cpm_id, output_dir)\n",
    "\n",
    "# Function to save attention weights for a specific `cpm_id`\n",
    "def output_attention_weights(all_attn_weights, graphs, cpm_id, output_dir):\n",
    "    for i, graph in enumerate(graphs):\n",
    "        if hasattr(graph, 'cpm_id') and graph.cpm_id == cpm_id:  # Find the graph with the specified `cpm_id`\n",
    "            attn_weights = all_attn_weights[i]\n",
    "            attn_weights_1, attn_weights_2, _ = attn_weights\n",
    "\n",
    "            # Convert attention weights to NumPy arrays for saving\n",
    "            attn_weights_1_array = [aw.cpu().numpy() for aw in attn_weights_1]\n",
    "            attn_weights_2_array = [aw.cpu().numpy() for aw in attn_weights_2]\n",
    "\n",
    "            # Extract node names\n",
    "            node_names = graph.node_names  # Ensure this attribute is available\n",
    "\n",
    "            # Transpose first attention weight array for easier access to node pairs\n",
    "            transposed_0 = attn_weights_1_array[0].T  # Now shaped (num_nodes, num_heads)\n",
    "\n",
    "            # Map indices to corresponding node names for readability\n",
    "            corresponding_node_names = []\n",
    "            for index_pair in transposed_0:\n",
    "                name_pair = [node_names[int(idx)] for idx in index_pair]\n",
    "                corresponding_node_names.append(name_pair)\n",
    "\n",
    "            # Convert to NumPy array\n",
    "            corresponding_node_names = np.array(corresponding_node_names)\n",
    "\n",
    "            # Combine node names and attention weights for saving\n",
    "            merged_array = np.column_stack((corresponding_node_names, attn_weights_1_array[1]))\n",
    "\n",
    "            # Save as CSV\n",
    "            np.savetxt(os.path.join(output_dir, f'{cpm_id}_attn_weights_1-multi_attention.csv'), merged_array, delimiter=',', fmt='%s')\n",
    "            print(f\"Attention weights attn_weights_1 saved as {cpm_id}_attn_weights_1-multi_attention.csv\")\n",
    "\n",
    "            # Repeat for `attn_weights_2` if needed\n",
    "            if attn_weights_2_array:\n",
    "                transposed_2 = attn_weights_2_array[0].T  # Transpose second array\n",
    "                corresponding_node_names_2 = []\n",
    "                for index_pair in transposed_2:\n",
    "                    name_pair = [node_names[int(idx)] for idx in index_pair]\n",
    "                    corresponding_node_names_2.append(name_pair)\n",
    "\n",
    "                corresponding_node_names_2 = np.array(corresponding_node_names_2)\n",
    "                merged_array_2 = np.column_stack((corresponding_node_names_2, attn_weights_2_array[1]))\n",
    "\n",
    "                # Save as CSV\n",
    "                np.savetxt(os.path.join(output_dir, f'{cpm_id}_attn_weights_2-multi_attention.csv'), merged_array_2, delimiter=',', fmt='%s')\n",
    "                print(f\"Attention weights attn_weights_2 saved as {cpm_id}_attn_weights_2-multi_attention.csv\")\n",
    "\n",
    "# Function to compute and save model performance metrics\n",
    "def compute_and_save_metrics(labels, outputs, output_dir, data_name):\n",
    "    num_classes = labels.shape[1]\n",
    "    metrics = {\n",
    "        'Class': [],\n",
    "        'Precision': [],\n",
    "        'Recall': [],\n",
    "        'F1 Score': [],\n",
    "        'AUC': [],\n",
    "        'Accuracy': [],\n",
    "        'Specificity': []\n",
    "    }\n",
    "    roc_data_long_format = {'Class': [], 'Reference': [], 'Predicted': []}\n",
    "    \n",
    "    for i in range(num_classes):\n",
    "        # Apply sigmoid to convert logits to probabilities\n",
    "        probabilities = torch.sigmoid(torch.tensor(outputs))\n",
    "        \n",
    "        # ROC curve and AUC calculation\n",
    "        fpr, tpr, thresholds = roc_curve(labels[:, i], probabilities[:, i].numpy())\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        # Store ROC data in long format for each class\n",
    "        for ref, pred in zip(labels[:, i], probabilities[:, i].numpy()):\n",
    "            roc_data_long_format['Class'].append(f'Class_{i+1}')\n",
    "            roc_data_long_format['Reference'].append(ref)\n",
    "            roc_data_long_format['Predicted'].append(pred)\n",
    "        \n",
    "        # Calculate Precision, Recall, F1, Accuracy, Specificity\n",
    "        pred_binary = (probabilities[:, i] > 0.5).numpy().astype(int)  # Binary predictions with threshold 0.5\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(labels[:, i], pred_binary, average='binary')\n",
    "        accuracy = accuracy_score(labels[:, i], pred_binary)\n",
    "        \n",
    "        tn, fp, fn, tp = confusion_matrix(labels[:, i], pred_binary).ravel()\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0  # Avoid division by zero\n",
    "\n",
    "        # Store metrics for each class\n",
    "        metrics['Class'].append(f'Class_{i+1}')\n",
    "        metrics['Precision'].append(precision)\n",
    "        metrics['Recall'].append(recall)\n",
    "        metrics['F1 Score'].append(f1)\n",
    "        metrics['AUC'].append(roc_auc)\n",
    "        metrics['Accuracy'].append(accuracy)\n",
    "        metrics['Specificity'].append(specificity)\n",
    "    \n",
    "    # Calculate average metrics across all classes\n",
    "    avg_metrics = {\n",
    "        'Class': ['Average'],\n",
    "        'Precision': [np.mean(metrics['Precision'])],\n",
    "        'Recall': [np.mean(metrics['Recall'])],\n",
    "        'F1 Score': [np.mean(metrics['F1 Score'])],\n",
    "        'AUC': [np.mean(metrics['AUC'])],\n",
    "        'Accuracy': [np.mean(metrics['Accuracy'])],\n",
    "        'Specificity': [np.mean(metrics['Specificity'])]\n",
    "    }\n",
    "    \n",
    "    # Append average metrics to the metrics dictionary\n",
    "    for key in metrics:\n",
    "        metrics[key].append(avg_metrics[key][0])\n",
    "    \n",
    "    # Save ROC data in long format to CSV\n",
    "    roc_df_long = pd.DataFrame(roc_data_long_format)\n",
    "    roc_df_long.to_csv(os.path.join(output_dir, f'{data_name}_roc_data_multi_attention.csv'), index=False)\n",
    "\n",
    "    # Save metrics data to CSV\n",
    "    metrics_df = pd.DataFrame(metrics)\n",
    "    metrics_df.to_csv(os.path.join(output_dir, f'{data_name}_metrics_multi_attention.csv'), index=False)\n",
    "    print(f\"Metrics and ROC data saved to {output_dir}.\")\n",
    "\n",
    "# Set working directory and define paths for input/output data\n",
    "work_dir = os.getcwd()  # Use current directory as work_dir\n",
    "input_data_dir = os.path.join(work_dir, '../Data')  # Set ../Data as input data location\n",
    "output_dir = os.path.join(work_dir, '../Data')  # Set ../Data as output data location\n",
    "\n",
    "# Evaluate on different datasets\n",
    "# Uncomment as needed\n",
    "# evaluate_model(train_graphs, train_labels, model, output_dir, \"train\")\n",
    "# evaluate_model(val_graphs, val_labels, model, output_dir, \"validation\")\n",
    "\n",
    "# Evaluate test set and optionally output attention weights for a specific `cpm_id`\n",
    "# evaluate_model(test_graphs, test_labels, model, output_dir, \"test-0\", cpm_id='CPM05651')\n",
    "evaluate_model(test_graphs, test_labels, model, output_dir, \"test\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
